{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9263f2-603d-4181-a123-2a898c3575bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/xrg8ngs979z999th3_nhl6sm0000gn/T/ipykernel_66308/2422631196.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"superres_unet_v4.pth\", map_location=\"cpu\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for UNet2D:\n\tMissing key(s) in state_dict: \"inc.double_conv.0.weight\", \"inc.double_conv.0.bias\", \"inc.double_conv.1.weight\", \"inc.double_conv.1.bias\", \"inc.double_conv.1.running_mean\", \"inc.double_conv.1.running_var\", \"inc.double_conv.3.weight\", \"inc.double_conv.3.bias\", \"inc.double_conv.4.weight\", \"inc.double_conv.4.bias\", \"inc.double_conv.4.running_mean\", \"inc.double_conv.4.running_var\", \"down1.1.double_conv.0.weight\", \"down1.1.double_conv.0.bias\", \"down1.1.double_conv.1.weight\", \"down1.1.double_conv.1.bias\", \"down1.1.double_conv.1.running_mean\", \"down1.1.double_conv.1.running_var\", \"down1.1.double_conv.3.weight\", \"down1.1.double_conv.3.bias\", \"down1.1.double_conv.4.weight\", \"down1.1.double_conv.4.bias\", \"down1.1.double_conv.4.running_mean\", \"down1.1.double_conv.4.running_var\", \"down2.1.double_conv.0.weight\", \"down2.1.double_conv.0.bias\", \"down2.1.double_conv.1.weight\", \"down2.1.double_conv.1.bias\", \"down2.1.double_conv.1.running_mean\", \"down2.1.double_conv.1.running_var\", \"down2.1.double_conv.3.weight\", \"down2.1.double_conv.3.bias\", \"down2.1.double_conv.4.weight\", \"down2.1.double_conv.4.bias\", \"down2.1.double_conv.4.running_mean\", \"down2.1.double_conv.4.running_var\", \"down3.1.double_conv.0.weight\", \"down3.1.double_conv.0.bias\", \"down3.1.double_conv.1.weight\", \"down3.1.double_conv.1.bias\", \"down3.1.double_conv.1.running_mean\", \"down3.1.double_conv.1.running_var\", \"down3.1.double_conv.3.weight\", \"down3.1.double_conv.3.bias\", \"down3.1.double_conv.4.weight\", \"down3.1.double_conv.4.bias\", \"down3.1.double_conv.4.running_mean\", \"down3.1.double_conv.4.running_var\", \"down4.1.double_conv.0.weight\", \"down4.1.double_conv.0.bias\", \"down4.1.double_conv.1.weight\", \"down4.1.double_conv.1.bias\", \"down4.1.double_conv.1.running_mean\", \"down4.1.double_conv.1.running_var\", \"down4.1.double_conv.3.weight\", \"down4.1.double_conv.3.bias\", \"down4.1.double_conv.5.weight\", \"down4.1.double_conv.5.bias\", \"down4.1.double_conv.5.running_mean\", \"down4.1.double_conv.5.running_var\", \"conv_up1.double_conv.1.weight\", \"conv_up1.double_conv.1.bias\", \"conv_up1.double_conv.1.running_mean\", \"conv_up1.double_conv.1.running_var\", \"conv_up1.double_conv.3.weight\", \"conv_up1.double_conv.3.bias\", \"conv_up1.double_conv.4.weight\", \"conv_up1.double_conv.4.bias\", \"conv_up1.double_conv.4.running_mean\", \"conv_up1.double_conv.4.running_var\", \"conv_up2.double_conv.1.weight\", \"conv_up2.double_conv.1.bias\", \"conv_up2.double_conv.1.running_mean\", \"conv_up2.double_conv.1.running_var\", \"conv_up2.double_conv.3.weight\", \"conv_up2.double_conv.3.bias\", \"conv_up2.double_conv.4.weight\", \"conv_up2.double_conv.4.bias\", \"conv_up2.double_conv.4.running_mean\", \"conv_up2.double_conv.4.running_var\", \"up3.weight\", \"up3.bias\", \"conv_up3.double_conv.0.weight\", \"conv_up3.double_conv.0.bias\", \"conv_up3.double_conv.1.weight\", \"conv_up3.double_conv.1.bias\", \"conv_up3.double_conv.1.running_mean\", \"conv_up3.double_conv.1.running_var\", \"conv_up3.double_conv.3.weight\", \"conv_up3.double_conv.3.bias\", \"conv_up3.double_conv.4.weight\", \"conv_up3.double_conv.4.bias\", \"conv_up3.double_conv.4.running_mean\", \"conv_up3.double_conv.4.running_var\", \"up4.weight\", \"up4.bias\", \"conv_up4.double_conv.0.weight\", \"conv_up4.double_conv.0.bias\", \"conv_up4.double_conv.1.weight\", \"conv_up4.double_conv.1.bias\", \"conv_up4.double_conv.1.running_mean\", \"conv_up4.double_conv.1.running_var\", \"conv_up4.double_conv.3.weight\", \"conv_up4.double_conv.3.bias\", \"conv_up4.double_conv.4.weight\", \"conv_up4.double_conv.4.bias\", \"conv_up4.double_conv.4.running_mean\", \"conv_up4.double_conv.4.running_var\", \"outc.weight\", \"outc.bias\". \n\tUnexpected key(s) in state_dict: \"conv_down1.double_conv.0.weight\", \"conv_down1.double_conv.0.bias\", \"conv_down1.double_conv.2.weight\", \"conv_down1.double_conv.2.bias\", \"conv_down2.double_conv.0.weight\", \"conv_down2.double_conv.0.bias\", \"conv_down2.double_conv.2.weight\", \"conv_down2.double_conv.2.bias\", \"conv_bottom.double_conv.0.weight\", \"conv_bottom.double_conv.0.bias\", \"conv_bottom.double_conv.2.weight\", \"conv_bottom.double_conv.2.bias\", \"conv_out.weight\", \"conv_out.bias\", \"conv_up1.double_conv.2.weight\", \"conv_up1.double_conv.2.bias\", \"conv_up2.double_conv.2.weight\", \"conv_up2.double_conv.2.bias\". \n\tsize mismatch for up1.weight: copying a param with shape torch.Size([128, 64, 2, 2]) from checkpoint, the shape in current model is torch.Size([1024, 512, 2, 2]).\n\tsize mismatch for up1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for conv_up1.double_conv.0.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 3, 3]).\n\tsize mismatch for conv_up1.double_conv.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up2.weight: copying a param with shape torch.Size([256, 128, 2, 2]) from checkpoint, the shape in current model is torch.Size([512, 256, 2, 2]).\n\tsize mismatch for up2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_up2.double_conv.0.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3]).\n\tsize mismatch for conv_up2.double_conv.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved super-resolved volume to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 82\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 64\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet2D(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Adjust path to your saved weights\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuperres_unet_v4.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# 2) Load the LR volume you want to upsample\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#    Make sure this LR volume is pre-resampled to match the desired shape in 3D (if needed).\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SR-Unet-MRI/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UNet2D:\n\tMissing key(s) in state_dict: \"inc.double_conv.0.weight\", \"inc.double_conv.0.bias\", \"inc.double_conv.1.weight\", \"inc.double_conv.1.bias\", \"inc.double_conv.1.running_mean\", \"inc.double_conv.1.running_var\", \"inc.double_conv.3.weight\", \"inc.double_conv.3.bias\", \"inc.double_conv.4.weight\", \"inc.double_conv.4.bias\", \"inc.double_conv.4.running_mean\", \"inc.double_conv.4.running_var\", \"down1.1.double_conv.0.weight\", \"down1.1.double_conv.0.bias\", \"down1.1.double_conv.1.weight\", \"down1.1.double_conv.1.bias\", \"down1.1.double_conv.1.running_mean\", \"down1.1.double_conv.1.running_var\", \"down1.1.double_conv.3.weight\", \"down1.1.double_conv.3.bias\", \"down1.1.double_conv.4.weight\", \"down1.1.double_conv.4.bias\", \"down1.1.double_conv.4.running_mean\", \"down1.1.double_conv.4.running_var\", \"down2.1.double_conv.0.weight\", \"down2.1.double_conv.0.bias\", \"down2.1.double_conv.1.weight\", \"down2.1.double_conv.1.bias\", \"down2.1.double_conv.1.running_mean\", \"down2.1.double_conv.1.running_var\", \"down2.1.double_conv.3.weight\", \"down2.1.double_conv.3.bias\", \"down2.1.double_conv.4.weight\", \"down2.1.double_conv.4.bias\", \"down2.1.double_conv.4.running_mean\", \"down2.1.double_conv.4.running_var\", \"down3.1.double_conv.0.weight\", \"down3.1.double_conv.0.bias\", \"down3.1.double_conv.1.weight\", \"down3.1.double_conv.1.bias\", \"down3.1.double_conv.1.running_mean\", \"down3.1.double_conv.1.running_var\", \"down3.1.double_conv.3.weight\", \"down3.1.double_conv.3.bias\", \"down3.1.double_conv.4.weight\", \"down3.1.double_conv.4.bias\", \"down3.1.double_conv.4.running_mean\", \"down3.1.double_conv.4.running_var\", \"down4.1.double_conv.0.weight\", \"down4.1.double_conv.0.bias\", \"down4.1.double_conv.1.weight\", \"down4.1.double_conv.1.bias\", \"down4.1.double_conv.1.running_mean\", \"down4.1.double_conv.1.running_var\", \"down4.1.double_conv.3.weight\", \"down4.1.double_conv.3.bias\", \"down4.1.double_conv.5.weight\", \"down4.1.double_conv.5.bias\", \"down4.1.double_conv.5.running_mean\", \"down4.1.double_conv.5.running_var\", \"conv_up1.double_conv.1.weight\", \"conv_up1.double_conv.1.bias\", \"conv_up1.double_conv.1.running_mean\", \"conv_up1.double_conv.1.running_var\", \"conv_up1.double_conv.3.weight\", \"conv_up1.double_conv.3.bias\", \"conv_up1.double_conv.4.weight\", \"conv_up1.double_conv.4.bias\", \"conv_up1.double_conv.4.running_mean\", \"conv_up1.double_conv.4.running_var\", \"conv_up2.double_conv.1.weight\", \"conv_up2.double_conv.1.bias\", \"conv_up2.double_conv.1.running_mean\", \"conv_up2.double_conv.1.running_var\", \"conv_up2.double_conv.3.weight\", \"conv_up2.double_conv.3.bias\", \"conv_up2.double_conv.4.weight\", \"conv_up2.double_conv.4.bias\", \"conv_up2.double_conv.4.running_mean\", \"conv_up2.double_conv.4.running_var\", \"up3.weight\", \"up3.bias\", \"conv_up3.double_conv.0.weight\", \"conv_up3.double_conv.0.bias\", \"conv_up3.double_conv.1.weight\", \"conv_up3.double_conv.1.bias\", \"conv_up3.double_conv.1.running_mean\", \"conv_up3.double_conv.1.running_var\", \"conv_up3.double_conv.3.weight\", \"conv_up3.double_conv.3.bias\", \"conv_up3.double_conv.4.weight\", \"conv_up3.double_conv.4.bias\", \"conv_up3.double_conv.4.running_mean\", \"conv_up3.double_conv.4.running_var\", \"up4.weight\", \"up4.bias\", \"conv_up4.double_conv.0.weight\", \"conv_up4.double_conv.0.bias\", \"conv_up4.double_conv.1.weight\", \"conv_up4.double_conv.1.bias\", \"conv_up4.double_conv.1.running_mean\", \"conv_up4.double_conv.1.running_var\", \"conv_up4.double_conv.3.weight\", \"conv_up4.double_conv.3.bias\", \"conv_up4.double_conv.4.weight\", \"conv_up4.double_conv.4.bias\", \"conv_up4.double_conv.4.running_mean\", \"conv_up4.double_conv.4.running_var\", \"outc.weight\", \"outc.bias\". \n\tUnexpected key(s) in state_dict: \"conv_down1.double_conv.0.weight\", \"conv_down1.double_conv.0.bias\", \"conv_down1.double_conv.2.weight\", \"conv_down1.double_conv.2.bias\", \"conv_down2.double_conv.0.weight\", \"conv_down2.double_conv.0.bias\", \"conv_down2.double_conv.2.weight\", \"conv_down2.double_conv.2.bias\", \"conv_bottom.double_conv.0.weight\", \"conv_bottom.double_conv.0.bias\", \"conv_bottom.double_conv.2.weight\", \"conv_bottom.double_conv.2.bias\", \"conv_out.weight\", \"conv_out.bias\", \"conv_up1.double_conv.2.weight\", \"conv_up1.double_conv.2.bias\", \"conv_up2.double_conv.2.weight\", \"conv_up2.double_conv.2.bias\". \n\tsize mismatch for up1.weight: copying a param with shape torch.Size([128, 64, 2, 2]) from checkpoint, the shape in current model is torch.Size([1024, 512, 2, 2]).\n\tsize mismatch for up1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for conv_up1.double_conv.0.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 3, 3]).\n\tsize mismatch for conv_up1.double_conv.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up2.weight: copying a param with shape torch.Size([256, 128, 2, 2]) from checkpoint, the shape in current model is torch.Size([512, 256, 2, 2]).\n\tsize mismatch for up2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_up2.double_conv.0.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 3, 3]).\n\tsize mismatch for conv_up2.double_conv.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256])."
     ]
    }
   ],
   "source": [
    "# inference.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "from models.model import UNet2D  # Adjust if your model file/path is different\n",
    "\n",
    "def load_nii_as_numpy(filepath):\n",
    "    \"\"\"\n",
    "    Load a NIfTI file and return the NumPy array and affine.\n",
    "    \"\"\"\n",
    "    nii_img = nib.load(filepath)\n",
    "    data = nii_img.get_fdata()  # shape [D, H, W] (or [H, W, D], depending on orientation)\n",
    "    affine = nii_img.affine     # to preserve the spatial orientation\n",
    "    return data, affine\n",
    "\n",
    "def predict_volume_2d(model, volume_3d):\n",
    "    \"\"\"\n",
    "    Given a 3D volume (NumPy array) and a 2D model:\n",
    "    1. Slice the volume along the first dimension [D, H, W].\n",
    "    2. Pass each slice through the model.\n",
    "    3. Return the predicted 3D volume.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # set model to eval mode\n",
    "\n",
    "    depth = volume_3d.shape[0]\n",
    "    preds_3d = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(depth):\n",
    "            # Extract a single slice: shape [H, W]\n",
    "            slice_2d = volume_3d[i, :, :].astype(np.float32)\n",
    "\n",
    "            # (Optional) Normalize or apply the same preprocessing as in training\n",
    "            # e.g.: slice_2d = (slice_2d - slice_2d.mean()) / slice_2d.std()\n",
    "\n",
    "            # Add batch dimension and channel dimension => shape [1, 1, H, W]\n",
    "            input_tensor = torch.from_numpy(slice_2d).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            # Move to GPU if available\n",
    "            device = next(model.parameters()).device\n",
    "            input_tensor = input_tensor.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_tensor)  # shape [1, 1, H, W]\n",
    "\n",
    "            # Convert back to NumPy\n",
    "            output_slice = output.squeeze().cpu().numpy()\n",
    "\n",
    "            preds_3d.append(output_slice)\n",
    "\n",
    "    # Stack along the depth dimension => shape [D, H, W]\n",
    "    preds_3d = np.stack(preds_3d, axis=0)\n",
    "    return preds_3d\n",
    "\n",
    "def main():\n",
    "    # 1) Load your trained model\n",
    "    model = UNet2D(in_channels=1, out_channels=1)\n",
    "    # Adjust path to your saved weights\n",
    "    model.load_state_dict(torch.load(\"superres_unet_v4.pth\", map_location=\"cpu\"))\n",
    "    model.cuda() if torch.cuda.is_available() else None\n",
    "\n",
    "    # 2) Load the LR volume you want to upsample\n",
    "    #    Make sure this LR volume is pre-resampled to match the desired shape in 3D (if needed).\n",
    "    lr_path = \"data/sub-OAS30001_axial_upsampled.nii\"\n",
    "    lr_volume, lr_affine = load_nii_as_numpy(lr_path)  # shape [D, H, W]\n",
    "\n",
    "    # 3) Predict the super-resolved volume (using 2D slices)\n",
    "    sr_preds = predict_volume_2d(model, lr_volume)  # shape [D, H, W]\n",
    "\n",
    "    # 4) Save the predicted volume as a NIfTI\n",
    "    sr_nifti = nib.Nifti1Image(sr_preds, lr_affine)\n",
    "    out_path = \"data/sub-OAS30001_sr_prediction.nii\"\n",
    "    nib.save(sr_nifti, out_path)\n",
    "    print(f\"Saved super-resolved volume to: {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc14e3-958d-4893-b1b1-adb8c9500b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import torchio as tio  # for resampling\n",
    "\n",
    "from models.model import UNet2D  # Adjust if your model file/path is different\n",
    "\n",
    "def load_and_resample_nii(filepath, target_spacing):\n",
    "    \"\"\"\n",
    "    Load a NIfTI file using TorchIO and resample it to the target spacing.\n",
    "    \n",
    "    Parameters:\n",
    "        filepath (str): Path to the original LR NIfTI file.\n",
    "        target_spacing (tuple): Desired voxel spacing (e.g., (1.0, 1.0, 1.0)).\n",
    "        \n",
    "    Returns:\n",
    "        data (numpy.ndarray): The resampled volume with shape [D, H, W].\n",
    "        affine (numpy.ndarray): The affine matrix of the resampled image.\n",
    "    \"\"\"\n",
    "    image = tio.ScalarImage(filepath)\n",
    "    resample_transform = tio.Resample(target_spacing)\n",
    "    resampled_image = resample_transform(image)\n",
    "    # Convert the image to a NumPy array.\n",
    "    # TorchIO returns a 4D array with shape [C, D, H, W]; for a single channel, take the first index.\n",
    "    data = resampled_image.numpy()[0]  # shape: [D, H, W]\n",
    "    affine = resampled_image.affine\n",
    "    return data, affine\n",
    "\n",
    "def predict_volume_2d(model, volume_3d):\n",
    "    \"\"\"\n",
    "    Given a 3D volume (NumPy array) and a 2D model:\n",
    "      1. Slice the volume along the first dimension [D, H, W].\n",
    "      2. Pass each slice through the model.\n",
    "      3. Return the predicted 3D volume.\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to eval mode\n",
    "\n",
    "    depth = volume_3d.shape[0]\n",
    "    preds_3d = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(depth):\n",
    "            # Extract a single slice: shape [H, W]\n",
    "            slice_2d = volume_3d[i, :, :].astype(np.float32)\n",
    "\n",
    "            # (Optional) Normalize or apply the same preprocessing as in training\n",
    "            # e.g.: slice_2d = (slice_2d - slice_2d.mean()) / slice_2d.std()\n",
    "\n",
    "            # Add batch dimension and channel dimension => shape [1, 1, H, W]\n",
    "            input_tensor = torch.from_numpy(slice_2d).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            # Move to GPU if available\n",
    "            device = next(model.parameters()).device\n",
    "            input_tensor = input_tensor.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_tensor)  # shape [1, 1, H, W]\n",
    "\n",
    "            # Convert back to NumPy\n",
    "            output_slice = output.squeeze().cpu().numpy()\n",
    "\n",
    "            preds_3d.append(output_slice)\n",
    "\n",
    "    # Stack along the depth dimension => shape [D, H, W]\n",
    "    preds_3d = np.stack(preds_3d, axis=0)\n",
    "    return preds_3d\n",
    "\n",
    "def main():\n",
    "    # 1) Load your trained model\n",
    "    model = UNet2D(in_channels=1, out_channels=1)\n",
    "    # Adjust path to your saved weights\n",
    "    model.load_state_dict(torch.load(\"superres_unet_v2.pth\", map_location=\"cpu\"))\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    # 2) Load the original LR volume and resample on the fly.\n",
    "    # Since you didn't pre-resample, use the original LR file.\n",
    "    lr_path = \"data/sub-OAS30001_ses-d0129_run-01_T1w_axial_LR.nii.gz\"\n",
    "    # Define the target spacing (adjust these values to match your training resolution)\n",
    "    target_spacing = (1.0, 1.0, 1.0)\n",
    "    lr_volume, lr_affine = load_and_resample_nii(lr_path, target_spacing)  # shape [D, H, W]\n",
    "\n",
    "    # 3) Predict the super-resolved volume (using 2D slices)\n",
    "    sr_preds = predict_volume_2d(model, lr_volume)  # shape [D, H, W]\n",
    "\n",
    "    # 4) Save the predicted volume as a NIfTI file\n",
    "    sr_nifti = nib.Nifti1Image(sr_preds, lr_affine)\n",
    "    out_path = \"data/sub-OAS30001_sr_prediction.nii.gz\"\n",
    "    nib.save(sr_nifti, out_path)\n",
    "    print(f\"Saved super-resolved volume to: {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
