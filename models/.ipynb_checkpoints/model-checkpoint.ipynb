{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08c09e0-f900-42aa-913c-b843c9fccf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.py\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # def update_learning_rate(schedulers, val_loss):\n",
    "# #     \"\"\"\n",
    "# #     Update the learning rate for each scheduler.\n",
    "    \n",
    "# #     For schedulers of type ReduceLROnPlateau, use scheduler.step(val_loss),\n",
    "# #     otherwise, call scheduler.step() without arguments.\n",
    "    \n",
    "# #     Parameters:\n",
    "# #       schedulers (list): List of learning rate scheduler objects.\n",
    "# #       val_loss (float): The validation loss used by ReduceLROnPlateau.\n",
    "# #     \"\"\"\n",
    "# #     for scheduler in schedulers:\n",
    "# #         if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "# #             scheduler.step(val_loss)\n",
    "# #         else:\n",
    "# #             scheduler.step()\n",
    "\n",
    "# class DoubleConv(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Double convolution:\n",
    "#     Conv2D -> ReLU -> Conv2D -> ReLU\n",
    "#     \"\"\"\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(DoubleConv, self).__init__()\n",
    "#         self.double_conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.double_conv(x)\n",
    "\n",
    "# class UNet2D(nn.Module):\n",
    "#     def __init__(self, in_channels=1, out_channels=1):\n",
    "#         super(UNet2D, self).__init__()\n",
    "\n",
    "#         # Encoder\n",
    "#         self.conv_down1 = DoubleConv(in_channels, 64)\n",
    "#         self.pool1 = nn.MaxPool2d(2)\n",
    "#         self.conv_down2 = DoubleConv(64, 128)\n",
    "#         self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         self.conv_bottom = DoubleConv(128, 256)\n",
    "\n",
    "#         # Decoder\n",
    "#         self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "#         self.conv_up2 = DoubleConv(256, 128)\n",
    "#         self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "#         self.conv_up1 = DoubleConv(128, 64)\n",
    "\n",
    "#         # Final output\n",
    "#         self.conv_out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder\n",
    "#         x1 = self.conv_down1(x)\n",
    "#         x2 = self.pool1(x1)\n",
    "#         x3 = self.conv_down2(x2)\n",
    "#         x4 = self.pool2(x3)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         x5 = self.conv_bottom(x4)\n",
    "\n",
    "#         # Decoder\n",
    "#         x6 = self.up2(x5)\n",
    "#         x6 = torch.cat([x6, x3], dim=1)  # skip connection\n",
    "#         x7 = self.conv_up2(x6)\n",
    "\n",
    "#         x8 = self.up1(x7)\n",
    "#         x8 = torch.cat([x8, x1], dim=1)  # skip connection\n",
    "#         x9 = self.conv_up1(x8)\n",
    "\n",
    "#         out = self.conv_out(x9)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9090886-829c-4dfe-9855-72d2462cd5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class DoubleConv(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A double convolution block:\n",
    "#       Conv2d -> BatchNorm2d -> ReLU -> Conv2d -> BatchNorm2d -> ReLU.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(DoubleConv, self).__init__()\n",
    "#         self.double_conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.double_conv(x)\n",
    "\n",
    "# class UNetDeep(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A deeper 2D U-Net model with four downsampling and four upsampling blocks.\n",
    "    \n",
    "#     Architecture:\n",
    "#       - Encoder:\n",
    "#           * inc: initial double conv (64 filters)\n",
    "#           * down1: 64 -> 128\n",
    "#           * down2: 128 -> 256\n",
    "#           * down3: 256 -> 512\n",
    "#           * down4: 512 -> 1024\n",
    "#       - Bottleneck: output of down4 (1024 channels)\n",
    "#       - Decoder:\n",
    "#           * up1: Transposed conv: 1024 -> 512, then concatenate with down3 output (512) to yield 1024 channels, then double conv → 512\n",
    "#           * up2: Transposed conv: 512 -> 256, then concatenate with down2 output (256) to yield 512 channels, then double conv → 256\n",
    "#           * up3: Transposed conv: 256 -> 128, then concatenate with down1 output (128) to yield 256 channels, then double conv → 128\n",
    "#           * up4: Transposed conv: 128 -> 64, then concatenate with initial conv output (64) to yield 128 channels, then double conv → 64\n",
    "#       - Final 1x1 convolution reduces channels to desired output.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, in_channels=1, out_channels=1):\n",
    "#         super(UNetDeep, self).__init__()\n",
    "        \n",
    "#         # Encoder\n",
    "#         self.inc   = DoubleConv(in_channels, 64)\n",
    "#         self.down1 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConv(64, 128)\n",
    "#         )\n",
    "#         self.down2 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConv(128, 256)\n",
    "#         )\n",
    "#         self.down3 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConv(256, 512)\n",
    "#         )\n",
    "#         self.down4 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConv(512, 1024)\n",
    "#         )\n",
    "        \n",
    "#         # Decoder\n",
    "#         self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "#         self.conv_up1 = DoubleConv(1024, 512)  # Concatenate with down3 (512)\n",
    "        \n",
    "#         self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "#         # self.conv_up2 = DoubleConv(512, 256)   # Concatenate with down2 (256)\n",
    "        \n",
    "#         self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "#         self.conv_up3 = DoubleConv(256, 128)   # Concatenate with down1 (128)\n",
    "        \n",
    "#         self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "#         self.conv_up4 = DoubleConv(128, 64)    # Concatenate with inc (64)\n",
    "        \n",
    "#         self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Encoder path\n",
    "#         x1 = self.inc(x)         # shape: (B, 64, H, W)\n",
    "#         x2 = self.down1(x1)      # shape: (B, 128, H/2, W/2)\n",
    "#         x3 = self.down2(x2)      # shape: (B, 256, H/4, W/4)\n",
    "#         x4 = self.down3(x3)      # shape: (B, 512, H/8, W/8)\n",
    "#         x5 = self.down4(x4)      # shape: (B, 1024, H/16, W/16)\n",
    "        \n",
    "#         # Decoder path with skip connections\n",
    "#         x = self.up1(x5)         # shape: (B, 512, H/8, W/8)\n",
    "#         x = torch.cat([x, x4], dim=1)  # (B, 1024, H/8, W/8)\n",
    "#         x = self.conv_up1(x)     # (B, 512, H/8, W/8)\n",
    "        \n",
    "#         x = self.up2(x)          # (B, 256, H/4, W/4)\n",
    "#         x = torch.cat([x, x3], dim=1)  # (B, 512, H/4, W/4)\n",
    "#         x = self.conv_up2(x)     # (B, 256, H/4, W/4)\n",
    "        \n",
    "#         x = self.up3(x)          # (B, 128, H/2, W/2)\n",
    "#         x = torch.cat([x, x2], dim=1)  # (B, 256, H/2, W/2)\n",
    "#         x = self.conv_up3(x)     # (B, 128, H/2, W/2)\n",
    "        \n",
    "#         x = self.up4(x)          # (B, 64, H, W)\n",
    "#         x = torch.cat([x, x1], dim=1)  # (B, 128, H, W)\n",
    "#         x = self.conv_up4(x)     # (B, 64, H, W)\n",
    "        \n",
    "#         x = self.outc(x)         # (B, out_channels, H, W)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d9836-64db-4edb-8ea1-8b446477b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DoubleConvComplex(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A double convolution block with Batch Normalization.\n",
    "#     Optionally, you can add dropout if needed.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, in_channels, out_channels, dropout=False, p=0.5):\n",
    "#         super(DoubleConvComplex, self).__init__()\n",
    "#         layers = [\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         ]\n",
    "#         if dropout:\n",
    "#             layers.insert(4, nn.Dropout2d(p))  # Add dropout between convs if desired\n",
    "#         self.double_conv = nn.Sequential(*layers)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.double_conv(x)\n",
    "\n",
    "# class UNet2D(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A deeper, more complex 2D U-Net model with an extra down/up block,\n",
    "#     Batch Normalization, and dropout in the bottleneck.\n",
    "    \n",
    "#     Architecture:\n",
    "#       - Encoder:\n",
    "#           * inc: initial double conv (in_channels -> 64)\n",
    "#           * down1: 64 -> 128\n",
    "#           * down2: 128 -> 256\n",
    "#           * down3: 256 -> 512\n",
    "#           * down4: 512 -> 1024\n",
    "#       - Bottleneck: dropout applied to 1024 channels\n",
    "#       - Decoder:\n",
    "#           * up1: 1024 -> 512, concat with down3 output (512) → 1024 channels, double conv → 512\n",
    "#           * up2: 512 -> 256, concat with down2 output (256) → 512 channels, double conv → 256\n",
    "#           * up3: 256 -> 128, concat with down1 output (128) → 256 channels, double conv → 128\n",
    "#           * up4: 128 -> 64, concat with inc output (64) → 128 channels, double conv → 64\n",
    "#       - Final 1x1 conv outputs the final segmentation/regression map.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, in_channels=1, out_channels=1):\n",
    "#         super(UNet2D, self).__init__()\n",
    "        \n",
    "#         # Encoder\n",
    "#         self.inc   = DoubleConvComplex(in_channels, 64)\n",
    "#         self.down1 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConvComplex(64, 128)\n",
    "#         )\n",
    "#         self.down2 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConvComplex(128, 256)\n",
    "#         )\n",
    "#         self.down3 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConvComplex(256, 512)\n",
    "#         )\n",
    "#         self.down4 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConvComplex(512, 1024, dropout=True, p=0.5)\n",
    "#         )\n",
    "        \n",
    "#         # Decoder\n",
    "#         self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "#         self.conv_up1 = DoubleConvComplex(1024, 512)\n",
    "        \n",
    "#         self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "#         self.conv_up2 = DoubleConvComplex(512, 256)\n",
    "        \n",
    "#         self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "#         self.conv_up3 = DoubleConvComplex(256, 128)\n",
    "        \n",
    "#         self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "#         self.conv_up4 = DoubleConvComplex(128, 64)\n",
    "        \n",
    "#         # Final 1x1 convolution\n",
    "#         self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Encoder\n",
    "#         x1 = self.inc(x)         # shape: (B, 64, H, W)\n",
    "#         x2 = self.down1(x1)      # shape: (B, 128, H/2, W/2)\n",
    "#         x3 = self.down2(x2)      # shape: (B, 256, H/4, W/4)\n",
    "#         x4 = self.down3(x3)      # shape: (B, 512, H/8, W/8)\n",
    "#         x5 = self.down4(x4)      # shape: (B, 1024, H/16, W/16) with dropout\n",
    "        \n",
    "#         # Decoder\n",
    "#         x = self.up1(x5)         # shape: (B, 512, H/8, W/8)\n",
    "#         x = torch.cat([x, x4], dim=1)  # (B, 1024, H/8, W/8)\n",
    "#         x = self.conv_up1(x)     # (B, 512, H/8, W/8)\n",
    "        \n",
    "#         x = self.up2(x)          # (B, 256, H/4, W/4)\n",
    "#         x = torch.cat([x, x3], dim=1)  # (B, 512, H/4, W/4)\n",
    "#         x = self.conv_up2(x)     # (B, 256, H/4, W/4)\n",
    "        \n",
    "#         x = self.up3(x)          # (B, 128, H/2, W/2)\n",
    "#         x = torch.cat([x, x2], dim=1)  # (B, 256, H/2, W/2)\n",
    "#         x = self.conv_up3(x)     # (B, 128, H/2, W/2)\n",
    "        \n",
    "#         x = self.up4(x)          # (B, 64, H, W)\n",
    "#         x = torch.cat([x, x1], dim=1)  # (B, 128, H, W)\n",
    "#         x = self.conv_up4(x)     # (B, 64, H, W)\n",
    "        \n",
    "#         x = self.outc(x)         # (B, out_channels, H, W)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855d6ea-a413-4905-aa9d-851788508e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.py\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class DoubleConvComplex(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A double convolution block with Batch Normalization.\n",
    "#     Optionally, you can add dropout if needed.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, in_channels, out_channels, dropout=False, p=0.5):\n",
    "#         super(DoubleConvComplex, self).__init__()\n",
    "#         layers = [\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(out_channels),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         ]\n",
    "#         if dropout:\n",
    "#             layers.insert(4, nn.Dropout2d(p))  # Add dropout between convs if desired\n",
    "#         self.double_conv = nn.Sequential(*layers)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.double_conv(x)\n",
    "\n",
    "# class UNet2D(nn.Module):\n",
    "#     \"\"\"\n",
    "#     A deeper, more complex 2D U-Net model with an extra down/up block,\n",
    "#     Batch Normalization, and dropout in the bottleneck.\n",
    "    \n",
    "#     Architecture:\n",
    "#       - Encoder:\n",
    "#           * inc: initial double conv (in_channels -> 64)\n",
    "#           * down1: 64 -> 128\n",
    "#           * down2: 128 -> 256\n",
    "#           * down3: 256 -> 512\n",
    "#           * down4: 512 -> 1024\n",
    "#       - Bottleneck: dropout applied to 1024 channels\n",
    "#       - Decoder:\n",
    "#           * up1: 1024 -> 512, concat with down3 output (512) → 1024 channels, double conv → 512\n",
    "#           * up2: 512 -> 256, concat with down2 output (256) → 512 channels, double conv → 256\n",
    "#           * up3: 256 -> 128, concat with down1 output (128) → 256 channels, double conv → 128\n",
    "#           * up4: 128 -> 64, concat with inc output (64) → 128 channels, double conv → 64\n",
    "#       - Final 1x1 conv outputs the final segmentation/regression map.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, in_channels=1, out_channels=1):\n",
    "#         super(UNet2D, self).__init__()\n",
    "        \n",
    "#         # Encoder\n",
    "#         self.inc   = DoubleConvComplex(in_channels, 64)\n",
    "#         self.down1 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConvComplex(64, 128)\n",
    "#         )\n",
    "#         self.down2 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConvComplex(128, 256)\n",
    "#         )\n",
    "#         self.down3 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConvComplex(256, 512)\n",
    "#         )\n",
    "#         self.down4 = nn.Sequential(\n",
    "#             nn.MaxPool2d(2),\n",
    "#             DoubleConvComplex(512, 1024, dropout=True, p=0.5)\n",
    "#         )\n",
    "        \n",
    "#         # Decoder\n",
    "#         self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "#         self.conv_up1 = DoubleConvComplex(1024, 512)\n",
    "        \n",
    "#         self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "#         self.conv_up2 = DoubleConvComplex(512, 256)\n",
    "        \n",
    "#         self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "#         self.conv_up3 = DoubleConvComplex(256, 128)\n",
    "        \n",
    "#         self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "#         self.conv_up4 = DoubleConvComplex(128, 64)\n",
    "        \n",
    "#         # Final 1x1 convolution\n",
    "#         self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Encoder\n",
    "#         x1 = self.inc(x)         # shape: (B, 64, H, W)\n",
    "#         x2 = self.down1(x1)      # shape: (B, 128, H/2, W/2)\n",
    "#         x3 = self.down2(x2)      # shape: (B, 256, H/4, W/4)\n",
    "#         x4 = self.down3(x3)      # shape: (B, 512, H/8, W/8)\n",
    "#         x5 = self.down4(x4)      # shape: (B, 1024, H/16, W/16) with dropout\n",
    "        \n",
    "#         # Decoder\n",
    "#         x = self.up1(x5)         # shape: (B, 512, H/8, W/8)\n",
    "#         x = torch.cat([x, x4], dim=1)  # (B, 1024, H/8, W/8)\n",
    "#         x = self.conv_up1(x)     # (B, 512, H/8, W/8)\n",
    "        \n",
    "#         x = self.up2(x)          # (B, 256, H/4, W/4)\n",
    "#         x = torch.cat([x, x3], dim=1)  # (B, 512, H/4, W/4)\n",
    "#         x = self.conv_up2(x)     # (B, 256, H/4, W/4)\n",
    "        \n",
    "#         x = self.up3(x)          # (B, 128, H/2, W/2)\n",
    "#         x = torch.cat([x, x2], dim=1)  # (B, 256, H/2, W/2)\n",
    "#         x = self.conv_up3(x)     # (B, 128, H/2, W/2)\n",
    "        \n",
    "#         x = self.up4(x)          # (B, 64, H, W)\n",
    "#         x = torch.cat([x, x1], dim=1)  # (B, 128, H, W)\n",
    "#         x = self.conv_up4(x)     # (B, 64, H, W)\n",
    "        \n",
    "#         x = self.outc(x)         # (B, out_channels, H, W)\n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35636442-11f7-48c5-bcad-3c8b2fe7d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def update_learning_rate(schedulers, val_loss):\n",
    "#     \"\"\"\n",
    "#     Update the learning rate for each scheduler.\n",
    "    \n",
    "#     For schedulers of type ReduceLROnPlateau, use scheduler.step(val_loss),\n",
    "#     otherwise, call scheduler.step() without arguments.\n",
    "    \n",
    "#     Parameters:\n",
    "#       schedulers (list): List of learning rate scheduler objects.\n",
    "#       val_loss (float): The validation loss used by ReduceLROnPlateau.\n",
    "#     \"\"\"\n",
    "#     for scheduler in schedulers:\n",
    "#         if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "#             scheduler.step(val_loss)\n",
    "#         else:\n",
    "#             scheduler.step()\n",
    "\n",
    "# class DoubleConv(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Double convolution:\n",
    "#     Conv2D -> ReLU -> Conv2D -> ReLU\n",
    "#     \"\"\"\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(DoubleConv, self).__init__()\n",
    "#         self.double_conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.double_conv(x)\n",
    "\n",
    "# class UNet2D(nn.Module):\n",
    "#     def __init__(self, in_channels=1, out_channels=1):\n",
    "#         super(UNet2D, self).__init__()\n",
    "\n",
    "#         # Encoder\n",
    "#         self.conv_down1 = DoubleConv(in_channels, 64)\n",
    "#         self.pool1 = nn.MaxPool2d(2)\n",
    "#         self.conv_down2 = DoubleConv(64, 128)\n",
    "#         self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         self.conv_bottom = DoubleConv(128, 256)\n",
    "\n",
    "#         # Decoder\n",
    "#         self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "#         self.conv_up2 = DoubleConv(256, 128)\n",
    "#         self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "#         self.conv_up1 = DoubleConv(128, 64)\n",
    "\n",
    "#         # Final output\n",
    "#         self.conv_out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder\n",
    "#         x1 = self.conv_down1(x)\n",
    "#         x2 = self.pool1(x1)\n",
    "#         x3 = self.conv_down2(x2)\n",
    "#         x4 = self.pool2(x3)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         x5 = self.conv_bottom(x4)\n",
    "\n",
    "#         # Decoder\n",
    "#         x6 = self.up2(x5)\n",
    "#         x6 = torch.cat([x6, x3], dim=1)  # skip connection\n",
    "#         x7 = self.conv_up2(x6)\n",
    "\n",
    "#         x8 = self.up1(x7)\n",
    "#         x8 = torch.cat([x8, x1], dim=1)  # skip connection\n",
    "#         x9 = self.conv_up1(x8)\n",
    "\n",
    "#         out = self.conv_out(x9)\n",
    "#         return out\n",
    "\n",
    "class DoubleConvComplex(nn.Module):\n",
    "    \"\"\"\n",
    "    A double convolution block with Batch Normalization.\n",
    "    Optionally, you can add dropout if needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout=False, p=0.5):\n",
    "        super(DoubleConvComplex, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.insert(4, nn.Dropout2d(p))  # Add dropout between convs if desired\n",
    "        self.double_conv = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet2D(nn.Module):\n",
    "    \"\"\"\n",
    "    A deeper, more complex 2D U-Net model with an extra down/up block,\n",
    "    Batch Normalization, and dropout in the bottleneck.\n",
    "    \n",
    "    Architecture:\n",
    "      - Encoder:\n",
    "          * inc: initial double conv (in_channels -> 64)\n",
    "          * down1: 64 -> 128\n",
    "          * down2: 128 -> 256\n",
    "          * down3: 256 -> 512\n",
    "          * down4: 512 -> 1024\n",
    "      - Bottleneck: dropout applied to 1024 channels\n",
    "      - Decoder:\n",
    "          * up1: 1024 -> 512, concat with down3 output (512) → 1024 channels, double conv → 512\n",
    "          * up2: 512 -> 256, concat with down2 output (256) → 512 channels, double conv → 256\n",
    "          * up3: 256 -> 128, concat with down1 output (128) → 256 channels, double conv → 128\n",
    "          * up4: 128 -> 64, concat with inc output (64) → 128 channels, double conv → 64\n",
    "      - Final 1x1 conv outputs the final segmentation/regression map.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet2D, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.inc   = DoubleConvComplex(in_channels, 64)\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConvComplex(64, 128)\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConvComplex(128, 256)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConvComplex(256, 512)\n",
    "        )\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConvComplex(512, 1024, dropout=True, p=0.5)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv_up1 = DoubleConvComplex(1024, 512)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv_up2 = DoubleConvComplex(512, 256)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv_up3 = DoubleConvComplex(256, 128)\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv_up4 = DoubleConvComplex(128, 64)\n",
    "        \n",
    "        # Final 1x1 convolution\n",
    "        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)         # shape: (B, 64, H, W)\n",
    "        x2 = self.down1(x1)      # shape: (B, 128, H/2, W/2)\n",
    "        x3 = self.down2(x2)      # shape: (B, 256, H/4, W/4)\n",
    "        x4 = self.down3(x3)      # shape: (B, 512, H/8, W/8)\n",
    "        x5 = self.down4(x4)      # shape: (B, 1024, H/16, W/16) with dropout\n",
    "\n",
    "        \n",
    "        \n",
    "        # Decoder\n",
    "        x = self.up1(x5)         # shape: (B, 512, H/8, W/8)\n",
    "        x = torch.cat([x, x4], dim=1)  # (B, 1024, H/8, W/8)\n",
    "        x = self.conv_up1(x)     # (B, 512, H/8, W/8)\n",
    "        \n",
    "        x = self.up2(x)          # (B, 256, H/4, W/4)\n",
    "        x = torch.cat([x, x3], dim=1)  # (B, 512, H/4, W/4)\n",
    "        x = self.conv_up2(x)     # (B, 256, H/4, W/4)\n",
    "        \n",
    "        x = self.up3(x)          # (B, 128, H/2, W/2)\n",
    "        x = torch.cat([x, x2], dim=1)  # (B, 256, H/2, W/2)\n",
    "        x = self.conv_up3(x)     # (B, 128, H/2, W/2)\n",
    "        \n",
    "        x = self.up4(x)          # (B, 64, H, W)\n",
    "        x = torch.cat([x, x1], dim=1)  # (B, 128, H, W)\n",
    "        x = self.conv_up4(x)     # (B, 64, H, W)\n",
    "        \n",
    "        x = self.outc(x)         # (B, out_channels, H, W)\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782f32ab-4957-4771-8e23-62c337e66bbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUp1 x shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx5 shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x5\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, x5], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
